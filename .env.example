# SIP AI Assistant - Environment Configuration
# =============================================
# Services:
# - vLLM for LLM
# - Speaches for STT (Whisper) + TTS (Piper/Kokoro)

# Hugging Face Token (for downloading models)
HF_TOKEN=your_huggingface_token_here

# ===================
# LLM Configuration (vLLM)
# ===================
LLM_MODEL=meta-llama/Llama-3.1-70B-Instruct
LLM_BACKEND=vllm

# ===================
# Speaches API (Unified STT + TTS)
# ===================
# The Speaches container provides both Whisper STT and Piper/Kokoro TTS
# via OpenAI-compatible API endpoints

# Model unloading prevention (set in docker-compose.yml):
#   STT__TTL=-1   # Never unload STT model
#   TTS__TTL=-1   # Never unload TTS model
#   VAD__TTL=-1   # Never unload VAD model
# Default is 300 seconds (5 minutes) of idle time before unloading

# ===================
# Whisper STT Settings
# ===================
# Distilled models are much faster
WHISPER_MODEL=Systran/faster-distil-whisper-small.en
WHISPER_LANGUAGE=en

# STT Mode: "realtime" or "batch"
# - realtime: Uses WebRTC streaming via /v1/realtime endpoint (lower latency, default)
# - batch: Uses traditional file upload via /v1/audio/transcriptions (more compatible)
STT_MODE=realtime

# WebRTC STUN server for realtime mode (for NAT traversal)
WEBRTC_STUN_SERVER=stun:stun.l.google.com:19302

# Compute type for Whisper inference
# Options: auto, int8, int8_float16, int8_float32, float16, float32
# Use 'auto' if you get errors about unsupported compute types
# Use 'int8' for fastest inference on most hardware
WHISPER_COMPUTE_TYPE=auto

# ===================
# TTS Settings (via Speaches API)
# ===================
# TTS Model - Models are auto-downloaded on first use
# Recommended options:
#   speaches-ai/Kokoro-82M-v1.0-ONNX (default, high quality, ~24kHz)
#   speaches-ai/Kokoro-82M-v1.0-ONNX-int8 (faster, slightly lower quality)
#
# For Piper voices, use the full HuggingFace model path:
#   rhasspy/piper-voice-en_US-lessac-medium
#   rhasspy/piper-voice-en_US-amy-medium
#   rhasspy/piper-voice-en_GB-alan-medium
TTS_MODEL=speaches-ai/Kokoro-82M-v1.0-ONNX

# Voice options for Kokoro (see https://huggingface.co/hexgrad/Kokoro-82M):
#   af_heart, af_bella, af_nicole, af_sarah, af_sky (female)
#   am_adam, am_michael (male)
#
# Voice options for Piper (depends on model):
#   Usually the model only has one voice, so this can be left empty
TTS_VOICE=af_heart

# Response format (wav recommended for low latency)
TTS_RESPONSE_FORMAT=wav

# Speech speed (1.0 = normal)
TTS_SPEED=1.0

# ===================
# SIP Configuration
# ===================
SIP_USER=ai-assistant
SIP_PASSWORD=
SIP_DOMAIN=localhost
SIP_PORT=5060
SIP_REGISTRAR=

# Maximum simultaneous SIP calls (inbound + outbound combined)
# Allows answering incoming calls while outbound calls are in progress
SIP_MAX_CALLS=8

# ===================
# Voice Interaction
# ===================
# Silence timeout before processing speech (ms)
SILENCE_TIMEOUT_MS=500

# Barge-in detection settings
BARGE_IN_MIN_DURATION=400
BARGE_IN_ENERGY_THRESHOLD=2000

# ===================
# Callback Settings
# ===================
# How long to wait for callback to be answered (seconds)
CALLBACK_RING_TIMEOUT=30

# ===================
# Tempest Weather API
# ===================
# Get your station ID and API token from https://tempestwx.com/settings/tokens
TEMPEST_STATION_ID=
TEMPEST_API_TOKEN=

# ===================
# Logging
# ===================
LOG_LEVEL=INFO

# ===================
# API Server
# ===================
# Port for the REST API (outbound calls, health check)
API_PORT=8080

# ===================
# Call Queue (Redis)
# ===================
# Redis URL for call queue (optional - disables queue if not set)
REDIS_URL=redis://redis:6379/0

# Maximum concurrent outbound calls (default: 1)
CALL_QUEUE_MAX_CONCURRENT=1

# ===================
# OpenTelemetry (Observability)
# ===================
# Enable OpenTelemetry instrumentation (default: false)
# When enabled, sends traces, metrics, and logs to OTLP endpoint
OTEL_ENABLED=false

# OTLP endpoint for traces, metrics, and logs
OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317

# Service name for traces (default: sip-agent)
OTEL_SERVICE_NAME=sip-agent

# Additional resource attributes (comma-separated key=value pairs)
OTEL_RESOURCE_ATTRIBUTES=service.namespace=sip-ai,deployment.environment=docker

# Enable automatic Python logging instrumentation
OTEL_PYTHON_LOGGING_AUTO_INSTRUMENTATION_ENABLED=true

