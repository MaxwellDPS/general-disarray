"""
Low-Latency Audio Pipeline with Piper TTS
==========================================
All ML inference offloaded to dedicated API services:
- Whisper API (speaches/faster-whisper-server) for STT
- Piper TTS via Wyoming protocol for TTS

Piper is extremely fast - typically < 100ms for short phrases.
"""

import asyncio
import io
import logging
import time
import wave
from collections import deque
from dataclasses import dataclass
from typing import AsyncGenerator, Optional, Tuple

import httpx
import numpy as np

try:
    import webrtcvad
    VAD_AVAILABLE = True
except ImportError:
    VAD_AVAILABLE = False

try:
    import scipy.signal
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

from config import Config


logger = logging.getLogger(__name__)


# ============================================================================
# Latency Tracking
# ============================================================================

@dataclass
class LatencyMetrics:
    """Track latency at each stage."""
    vad_start: float = 0
    speech_end: float = 0
    stt_start: float = 0
    stt_end: float = 0
    llm_first_token: float = 0
    llm_complete: float = 0
    tts_first_chunk: float = 0
    audio_start: float = 0
    
    def log_summary(self):
        """Log latency breakdown."""
        if self.speech_end and self.stt_end:
            stt_latency = (self.stt_end - self.speech_end) * 1000
            logger.info(f"STT latency: {stt_latency:.0f}ms")
        if self.stt_end and self.llm_first_token:
            llm_ttft = (self.llm_first_token - self.stt_end) * 1000
            logger.info(f"LLM TTFT: {llm_ttft:.0f}ms")
        if self.llm_first_token and self.tts_first_chunk:
            tts_latency = (self.tts_first_chunk - self.llm_first_token) * 1000
            logger.info(f"TTS first chunk: {tts_latency:.0f}ms")
        if self.speech_end and self.audio_start:
            total = (self.audio_start - self.speech_end) * 1000
            logger.info(f"Total response latency: {total:.0f}ms")


# ============================================================================
# Optimized VAD with Shorter Timeouts
# ============================================================================

class FastVoiceActivityDetector:
    """
    Optimized VAD with aggressive silence detection.
    """
    
    def __init__(self, config: Config):
        self.config = config
        self.sample_rate = config.sample_rate
        
        self.vad = None
        if VAD_AVAILABLE:
            self.vad = webrtcvad.Vad(3)  # Mode 3 = most aggressive
            
        self.speech_frames = deque(maxlen=50)
        self.silence_frames = 0
        self.is_speaking = False
        
        self.noise_floor = 200
        self.noise_samples = deque(maxlen=100)
        
        self.silence_timeout_ms = config.silence_duration_ms
        
    def is_speech(self, audio_chunk: bytes) -> bool:
        """Check if chunk contains speech with energy pre-filter."""
        samples = np.frombuffer(audio_chunk, dtype=np.int16)
        energy = np.sqrt(np.mean(samples.astype(np.float32) ** 2))
        
        if not self.is_speaking:
            self.noise_samples.append(energy)
            if len(self.noise_samples) >= 10:
                self.noise_floor = np.percentile(list(self.noise_samples), 30)
        
        if energy < self.noise_floor * 1.5:
            return False
            
        if self.vad:
            try:
                frame_size = int(self.sample_rate * 0.03) * 2  # 30ms
                for i in range(0, len(audio_chunk), frame_size):
                    frame = audio_chunk[i:i + frame_size]
                    if len(frame) == frame_size:
                        if self.vad.is_speech(frame, self.sample_rate):
                            return True
                return False
            except:
                pass
                
        return energy > self.noise_floor * 2
        
    def process_audio(self, audio_chunk: bytes) -> Tuple[bool, bool]:
        """Process audio with faster end-of-utterance detection."""
        is_speech = self.is_speech(audio_chunk)
        
        if is_speech:
            self.speech_frames.append(audio_chunk)
            self.silence_frames = 0
            self.is_speaking = True
        else:
            self.silence_frames += 1
            
        silence_ms = self.silence_frames * self.config.chunk_duration_ms
        end_of_utterance = (
            self.is_speaking and 
            silence_ms >= self.silence_timeout_ms
        )
        
        if end_of_utterance:
            self.is_speaking = False
            
        return is_speech, end_of_utterance
        
    def reset(self):
        """Reset state."""
        self.speech_frames.clear()
        self.silence_frames = 0
        self.is_speaking = False


# ============================================================================
# Whisper API Client (OpenAI-compatible)
# ============================================================================

class WhisperAPIClient:
    """
    Whisper API client using OpenAI-compatible endpoints.
    """
    
    def __init__(self, config: Config):
        self.config = config
        self.base_url = config.whisper_api_url.rstrip('/')
        self.model = config.whisper_model
        self.language = config.whisper_language
        self.client: Optional[httpx.AsyncClient] = None
        self.available = False
        
    async def initialize(self):
        """Initialize the API client."""
        self.client = httpx.AsyncClient(timeout=60.0)
        
        try:
            response = await self.client.get(f"{self.base_url}/health")
            if response.status_code == 200:
                self.available = True
                logger.info(f"Whisper API available at {self.base_url}")
            else:
                logger.warning(f"Whisper API returned status {response.status_code}")
        except Exception as e:
            logger.warning(f"Whisper API not available: {e}")
            self.available = False
            
    async def close(self):
        """Close the client."""
        if self.client:
            await self.client.aclose()
            
    async def transcribe(self, audio_data: bytes) -> str:
        """
        Transcribe audio using OpenAI-compatible API.
        """
        if not self.available or not self.client:
            logger.warning("Whisper API not available")
            return ""
            
        try:
            wav_buffer = io.BytesIO()
            with wave.open(wav_buffer, 'wb') as wav:
                wav.setnchannels(1)
                wav.setsampwidth(2)
                wav.setframerate(self.config.sample_rate)
                wav.writeframes(audio_data)
            wav_buffer.seek(0)
            
            files = {
                'file': ('audio.wav', wav_buffer, 'audio/wav')
            }
            data = {
                'model': self.model,
                'language': self.language,
                'response_format': 'json'
            }
            
            response = await self.client.post(
                f"{self.base_url}/v1/audio/transcriptions",
                files=files,
                data=data
            )
            
            if response.status_code == 200:
                result = response.json()
                text = result.get('text', '').strip()
                return text
            else:
                logger.error(f"Whisper API error: {response.status_code} - {response.text}")
                return ""
                
        except Exception as e:
            logger.error(f"Transcription error: {e}")
            return ""


# ============================================================================
# Piper TTS Client (Wyoming Protocol)
# ============================================================================

class PiperClient:
    """
    Piper TTS client using Wyoming protocol with the wyoming library.
    
    Piper outputs 22050Hz 16-bit mono audio by default.
    """
    
    PIPER_SAMPLE_RATE = 22050
    
    def __init__(self, config: Config):
        self.config = config
        self.host = config.piper_host
        self.port = config.piper_port
        self.voice = config.piper_voice
        self.available = False
        
        # Audio cache for common phrases
        self.audio_cache: dict = {}
        self.cache_enabled = True
        
        # Wyoming imports
        self._wyoming_available = False
        try:
            from wyoming.client import AsyncClient
            from wyoming.tts import Synthesize
            from wyoming.audio import AudioChunk
            self._wyoming_available = True
        except ImportError:
            logger.warning("wyoming library not available")
        
    async def initialize(self):
        """Test connection to Piper."""
        if not self._wyoming_available:
            logger.error("Wyoming library not installed")
            return
            
        try:
            from wyoming.client import AsyncClient
            
            uri = f"tcp://{self.host}:{self.port}"
            async with AsyncClient.from_uri(uri) as client:
                # Connection successful
                pass
                
            self.available = True
            logger.info(f"Piper TTS available at {self.host}:{self.port}")
            
            # Pre-cache common phrases
            if self.cache_enabled:
                await self._precache_phrases()
                
        except Exception as e:
            logger.warning(f"Piper TTS not available: {e}")
            self.available = False
            
    async def _precache_phrases(self):
        """Pre-synthesize common acknowledgments."""
        phrases = [
            "Okay", "Sure", "Got it", "One moment",
            "Copy that", "On it", "Checking", "Let me see",
            "Working on it", "Goodbye", "Hello"
        ]
        
        logger.info("Pre-caching common phrases...")
        
        for phrase in phrases:
            try:
                audio = await self._synthesize_raw(phrase)
                if audio:
                    # Resample to target rate
                    audio = self._resample(audio, self.PIPER_SAMPLE_RATE, self.config.sample_rate)
                    self.audio_cache[phrase.lower()] = audio
            except Exception as e:
                logger.warning(f"Failed to cache '{phrase}': {e}")
                
        logger.info(f"Cached {len(self.audio_cache)} phrases")
        
    async def close(self):
        """Close client (no persistent connection)."""
        pass
        
    def get_cached(self, text: str) -> Optional[bytes]:
        """Get pre-cached audio if available."""
        return self.audio_cache.get(text.lower().strip())
        
    async def _synthesize_raw(self, text: str) -> bytes:
        """
        Synthesize text using Wyoming protocol with wyoming library.
        """
        if not self.available or not self._wyoming_available:
            return b''
            
        try:
            from wyoming.client import AsyncClient
            from wyoming.tts import Synthesize
            from wyoming.audio import AudioChunk
            
            uri = f"tcp://{self.host}:{self.port}"
            audio_chunks = []
            
            async with AsyncClient.from_uri(uri) as client:
                # Send synthesize request
                await client.write_event(Synthesize(text=text).event())
                
                # Read audio chunks
                while True:
                    event = await asyncio.wait_for(
                        client.read_event(),
                        timeout=30.0
                    )
                    
                    if event is None:
                        break
                        
                    if event.type == "audio-stop":
                        break
                        
                    if event.type == "audio-chunk":
                        chunk = AudioChunk.from_event(event)
                        audio_chunks.append(chunk.audio)
                        
            if audio_chunks:
                return b''.join(audio_chunks)
            return b''
            
        except asyncio.TimeoutError:
            logger.warning("Piper response timeout")
            return b''
        except Exception as e:
            logger.error(f"Piper synthesis error: {e}")
            return b''
            
    async def synthesize(self, text: str) -> bytes:
        """Synthesize with cache check and resampling."""
        # Check cache first
        cached = self.get_cached(text)
        if cached:
            logger.debug(f"Cache hit for: {text}")
            return cached
            
        if not self.available:
            logger.warning("Piper not available")
            return b''
            
        start = time.time()
        audio = await self._synthesize_raw(text)
        
        if audio:
            # Resample from Piper's 22050Hz to target rate (usually 16000Hz)
            audio = self._resample(audio, self.PIPER_SAMPLE_RATE, self.config.sample_rate)
            
            elapsed = (time.time() - start) * 1000
            logger.info(f"Piper TTS: {elapsed:.0f}ms for '{text[:30]}...'")
            
        return audio
        
    async def synthesize_stream(self, text: str) -> AsyncGenerator[bytes, None]:
        """
        Stream synthesis - Piper is fast enough that we can just
        synthesize the whole thing and yield it in chunks.
        """
        audio = await self.synthesize(text)
        
        if audio:
            # Yield in chunks for streaming playback
            chunk_size = 4096
            for i in range(0, len(audio), chunk_size):
                yield audio[i:i + chunk_size]
                
    def _resample(self, audio: bytes, from_rate: int, to_rate: int) -> bytes:
        """Resample audio to target sample rate."""
        if from_rate == to_rate:
            return audio
            
        samples = np.frombuffer(audio, dtype=np.int16)
        if len(samples) == 0:
            return audio
            
        if SCIPY_AVAILABLE:
            ratio = to_rate / from_rate
            new_len = int(len(samples) * ratio)
            resampled = scipy.signal.resample(samples.astype(np.float64), new_len)
        else:
            # Linear interpolation fallback
            ratio = to_rate / from_rate
            new_indices = np.linspace(0, len(samples) - 1, int(len(samples) * ratio))
            resampled = np.interp(new_indices, np.arange(len(samples)), samples.astype(np.float64))
            
        return resampled.astype(np.int16).tobytes()


# ============================================================================
# Optimized Audio Pipeline (API-based)
# ============================================================================

class LowLatencyAudioPipeline:
    """
    Low-latency audio pipeline using API-based STT and Piper TTS.
    
    Target latencies:
    - STT: < 300ms (via Whisper API)
    - LLM TTFT: < 500ms  
    - TTS: < 100ms (Piper is very fast)
    - Total: < 900ms
    """
    
    def __init__(self, config: Config):
        self.config = config
        
        # Components
        self.vad = FastVoiceActivityDetector(config)
        self.stt = WhisperAPIClient(config)
        self.tts = PiperClient(config)
        
        # Audio buffer
        self.audio_buffer = bytearray()
        self.max_buffer_size = int(config.max_speech_duration_s * config.sample_rate * 2)
        
        # Metrics
        self.last_metrics = LatencyMetrics()
        
    async def start(self):
        """Initialize all components."""
        logger.info("Starting low-latency audio pipeline...")
        
        start = time.time()
        
        # Initialize API clients in parallel
        await asyncio.gather(
            self.stt.initialize(),
            self.tts.initialize()
        )
        
        load_time = (time.time() - start) * 1000
        logger.info(f"Pipeline ready in {load_time:.0f}ms")
        
        if self.stt.available:
            logger.info(f"Whisper API ready at {self.config.whisper_api_url}")
        else:
            logger.warning("Whisper API not available")
            
        if self.tts.available:
            logger.info(f"Piper TTS ready, {len(self.tts.audio_cache)} phrases cached")
        else:
            logger.warning("Piper TTS not available")
            
    async def stop(self):
        """Cleanup."""
        await self.stt.close()
        await self.tts.close()
        
    async def process_audio(self, audio_chunk: bytes) -> Optional[str]:
        """Process audio with fast end-of-utterance detection."""
        is_speech, end_of_utterance = self.vad.process_audio(audio_chunk)
        
        if is_speech:
            self.audio_buffer.extend(audio_chunk)
            
            if len(self.audio_buffer) > self.max_buffer_size:
                logger.warning("Buffer overflow, forcing transcription")
                return await self._transcribe_buffer()
                
        if end_of_utterance and len(self.audio_buffer) > 0:
            return await self._transcribe_buffer()
            
        return None
        
    async def _transcribe_buffer(self) -> str:
        """Transcribe buffered audio via API."""
        self.last_metrics.speech_end = time.time()
        
        audio_data = bytes(self.audio_buffer)
        self.audio_buffer.clear()
        self.vad.reset()
        
        duration_ms = len(audio_data) / (self.config.sample_rate * 2) * 1000
        if duration_ms < self.config.min_speech_duration_ms:
            return ""
            
        self.last_metrics.stt_start = time.time()
        result = await self.stt.transcribe(audio_data)
        self.last_metrics.stt_end = time.time()
        
        stt_latency = (self.last_metrics.stt_end - self.last_metrics.speech_end) * 1000
        logger.info(f"STT API: {stt_latency:.0f}ms for {duration_ms:.0f}ms audio")
        
        return result
        
    async def synthesize(self, text: str) -> bytes:
        """Synthesize with caching."""
        return await self.tts.synthesize(text)
        
    async def synthesize_stream(self, text: str) -> AsyncGenerator[bytes, None]:
        """Stream synthesis."""
        async for chunk in self.tts.synthesize_stream(text):
            yield chunk
            
    def get_cached_audio(self, text: str) -> Optional[bytes]:
        """Get pre-cached audio for instant playback."""
        return self.tts.get_cached(text)
        
    def has_speech(self, audio_chunk: bytes) -> bool:
        """Quick speech check."""
        return self.vad.is_speech(audio_chunk)
