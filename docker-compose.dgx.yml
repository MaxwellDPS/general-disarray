# Docker Compose for SIP AI Assistant with Speaches (STT + TTS) + vLLM
# =====================================================================
# Optimized for GB10/Grace Blackwell with 100GB+ VRAM
#
# Services:
#   - vllm: LLM server (Llama 3.1 70B)
#   - speaches: Unified STT (Whisper) + TTS (Piper/Kokoro) server
#   - redis: Queue for outbound calls
#   - sip-agent: Main SIP application

services:
  # ============================================================================
  # Redis - Queue for outbound calls
  # ============================================================================
  redis:
    image: redis:7-alpine
    container_name: sip-ai-redis
    command: redis-server --appendonly yes
    volumes:
      - ./data/redis:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # ============================================================================
  # vLLM Server - Serves the LLM
  # ============================================================================
  vllm:
    image: nvcr.io/nvidia/vllm:25.11-py3
    container_name: sip-ai-vllm
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    command: >
      vllm serve
      ${LLM_MODEL:-meta-llama/Llama-3.1-70B-Instruct}
      --port 8000
      --gpu-memory-utilization 0.60
      --max-model-len 8192
      --trust-remote-code
      --enable-prefix-caching
      --enable-chunked-prefill
      --max-num-batched-tokens 2048
      --max-num-seqs 4
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  # ============================================================================
  # Speaches Server - Unified STT (Whisper) + TTS (Piper/Kokoro)
  # OpenAI-compatible API for both transcription and speech synthesis
  # ============================================================================
  speaches:
    image: ghcr.io/cha0s-corp/speaches-cuda:latest
    build:
      context: ./speaches/
      dockerfile: Dockerfile
    container_name: sip-ai-speaches
    runtime: nvidia
    ipc: host
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      # STT Settings (Whisper)
      - WHISPER__MODEL=${WHISPER_MODEL:-Systran/faster-distil-whisper-small.en}
      # Options: default, int8, int8_float16, int8_float32, int8_bfloat16, 
      #          int16, float16, bfloat16, float32
      - WHISPER__COMPUTE_TYPE=${WHISPER_COMPUTE_TYPE:-default}
      - WHISPER__INFERENCE_DEVICE=${WHISPER_DEVICE:-cuda}
      - WHISPER__INFERENCE_CONFIG__BEAM_SIZE=${WHISPER_BEAM_SIZE:-1}
      - WHISPER__INFERENCE_CONFIG__LANGUAGE=${WHISPER_LANGUAGE:-en}
      - WHISPER__INFERENCE_CONFIG__VAD_FILTER=true
      - WHISPER__INFERENCE_CONFIG__BEST_OF=1
      # Model TTL settings - prevent unloading (-1 = never unload)
      - STT_MODEL_TTL=${STT_MODEL_TTL:--1}
      - TTS_MODEL_TTL=${TTS_MODEL_TTL:--1}
      - VAD_MODEL_TTL=${VAD_MODEL_TTL:--1}
      - UVICORN_PORT=8001
      # TTS is configured via API request parameters (model/voice)
    ports:
      - "8001:8001"
    volumes:
      - ./cache/speaches-cache:/home/ubuntu/.cache/huggingface/hub
      - ./cache/piper-voices:/home/ubuntu/.local/share/piper_voices
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  # ============================================================================
  # SIP AI Assistant - Main application (lightweight, no local models)
  # ============================================================================
  sip-agent:
    # build:
    #   context: sip-agent/
    #   dockerfile: Dockerfile
    container_name: sip-agent
    image:  ghcr.io/cha0s-corp/sip-agent:latest
    depends_on:
      redis:
        condition: service_healthy
      vllm:
        condition: service_healthy
      speaches:
        condition: service_healthy
    environment:
      # Redis settings
      - REDIS_URL=redis://redis:6379/0
      # Call queue settings
      - CALL_QUEUE_MAX_CONCURRENT=${CALL_QUEUE_MAX_CONCURRENT:-1}
      # LLM settings
      - LLM_BACKEND=vllm
      - LLM_BASE_URL=http://vllm:8000/v1
      - LLM_MODEL=${LLM_MODEL:-meta-llama/Llama-3.1-70B-Instruct}
      - LLM_MAX_TOKENS=${LLM_MAX_TOKENS:-512}
      - LLM_TEMPERATURE=${LLM_TEMPERATURE:-0.6}
      - LLM_TOP_P=${LLM_TOP_P:-0.85}
      - LLM_API_KEY=${LLM_API_KEY:-}
      - MAX_CONVERSATION_TURNS=${MAX_CONVERSATION_TURNS:-10}
      # Speaches API settings (unified STT + TTS)
      - SPEACHES_API_URL=http://speaches:8001
      # STT settings
      - WHISPER_MODEL=${WHISPER_MODEL:-Systran/faster-distil-whisper-small.en}
      - WHISPER_LANGUAGE=${WHISPER_LANGUAGE:-en}
      - MIN_SPEECH_DURATION_MS=${MIN_SPEECH_DURATION_MS:-200}
      - MAX_SPEECH_DURATION_S=${MAX_SPEECH_DURATION_S:-8.0}
      - SILENCE_TIMEOUT_MS=${SILENCE_TIMEOUT_MS:-750}
      # TTS settings (Kokoro via Speaches - auto-downloaded on first use)
      - TTS_MODEL=${TTS_MODEL:-speaches-ai/Kokoro-82M-v1.0-ONNX}
      - TTS_VOICE=${TTS_VOICE:-af_heart}
      - TTS_RESPONSE_FORMAT=${TTS_RESPONSE_FORMAT:-wav}
      - TTS_SPEED=${TTS_SPEED:-1.0}
      # SIP settings
      - SIP_USER=${SIP_USER:-ai-assistant}
      - SIP_PASSWORD=${SIP_PASSWORD:-}
      - SIP_DOMAIN=${SIP_DOMAIN:-localhost}
      - SIP_PORT=${SIP_PORT:-5060}
      - SIP_REGISTRAR=${SIP_REGISTRAR:-}
      # General
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      # Tools
      - CALLBACK_RING_TIMEOUT=${CALLBACK_RING_TIMEOUT:-30}
      # Tempest Weather API (optional)
      - TEMPEST_STATION_ID=${TEMPEST_STATION_ID:-}
      - TEMPEST_API_TOKEN=${TEMPEST_API_TOKEN:-}
      # API settings
      - API_PORT=${API_PORT:-8080}
    ports:
      - "5060:5060/udp"
      - "5060:5060/tcp"
      - "10000-10100:10000-10100/udp"  # RTP ports
      - ${API_PORT:-8080}:8080                    # API server port
    volumes:
      - ./data:/app/data
    restart: unless-stopped

  n8n:
    image: n8nio/n8n:latest
    container_name: n8n
    restart: unless-stopped
    ports:
      - "5678:5678"
    environment:
      - N8N_HOST=0.0.0.0
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - WEBHOOK_URL=http://n8n:5678/
      - GENERIC_TIMEZONE=America/Los_Angeles
    volumes:
      - n8n_data:/home/node/.n8n

volumes:
  n8n_data:


# ============================================================================
# Networks
# ============================================================================
networks:
  default:
    driver: bridge
