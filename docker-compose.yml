# Docker Compose for SIP AI Assistant with Fish Speech + Whisper API
# ===================================================================
# Optimized for GB10/Grace Blackwell with 100GB+ VRAM
#
# Services:
#   - vllm: LLM server (Llama 3.1 70B)
#   - whisper: Faster-Whisper STT server (OpenAI-compatible API)
#   - fish-speech: Fish Speech / OpenAudio TTS server (streaming)
#   - sip-agent: Main SIP application

services:
  # ============================================================================
  # vLLM Server - Serves the LLM
  # ============================================================================
  vllm:
    image: nvcr.io/nvidia/vllm:25.11-py3
    container_name: sip-ai-vllm
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN:-}
    env_file:
      - .env
    ipc: host
    ulimits:
      memlock: -1
      stack: 67108864
    command: >
      vllm serve
      ${LLM_MODEL:-meta-llama/Llama-3.1-70B-Instruct}
      --port 8000
      --gpu-memory-utilization 0.50
      --max-model-len 8192
      --trust-remote-code
    ports:
      - "8000:8000"
    volumes:
      - ./cache/huggingface-cache:/root/.cache/huggingface
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  # ============================================================================
  # Whisper Server - Speech-to-Text (OpenAI-compatible API)
  # Uses speaches/faster-whisper-server
  # ============================================================================
  whisper:
    image: ghcr.io/speaches-ai/speaches:latest-cuda
    container_name: sip-ai-whisper
    runtime: nvidia
    ipc: host
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      # Use smaller/faster distilled model
      - WHISPER__MODEL=Systran/faster-distil-whisper-small.en
      - WHISPER__COMPUTE_TYPE=float32 
      - WHISPER__DEVICE=cuda
      # Faster inference settings
      - WHISPER__INFERENCE_CONFIG__BEAM_SIZE=1
      - WHISPER__INFERENCE_CONFIG__LANGUAGE=${WHISPER_LANGUAGE:-en}
      - WHISPER__INFERENCE_CONFIG__VAD_FILTER=true
      - WHISPER__INFERENCE_CONFIG__BEST_OF=1
    ports:
      - "8001:8000"
    volumes:
      - ./cache/whisper-cache:/home/ubuntu/.cache/huggingface/hub
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped

  # ============================================================================
  # Piper TTS - Fast local text-to-speech
  # https://github.com/rhasspy/piper
  # ============================================================================
  piper:
    image: rhasspy/wyoming-piper:latest
    container_name: sip-ai-piper
    command: --voice en_US-amy-medium
    ports:
      - "10200:10200"
    volumes:
      - ./cache/piper:/data
    # healthcheck:
    #   test: ["CMD-SHELL", "echo '{}' | nc -w 1 localhost 10200 || exit 1"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3
    #   start_period: 60s
    restart: unless-stopped

  # ============================================================================
  # SIP AI Assistant - Main application (lightweight, no local models)
  # ============================================================================
  sip-agent:
    build:
      context: sip-agent/
      dockerfile: Dockerfile
    container_name: sip-ai-assistant
    depends_on:
      vllm:
        condition: service_healthy
      whisper:
        condition: service_healthy
      # piper:
      #   condition: service_healthy
    environment:
      # LLM settings
      - LLM_BACKEND=vllm
      - LLM_BASE_URL=http://vllm:8000/v1
      - LLM_MODEL=${LLM_MODEL:-meta-llama/Llama-3.1-70B-Instruct}
      # Whisper API settings
      - WHISPER_API_URL=http://whisper:8000
      - WHISPER_MODEL=Systran/faster-distil-whisper-small.en
      - WHISPER_LANGUAGE=${WHISPER_LANGUAGE:-en}
      # Piper TTS settings
      - PIPER_HOST=piper
      - PIPER_PORT=10200
      - PIPER_VOICE=${PIPER_VOICE:-en_US-lessac-medium}
      # SIP settings
      - SIP_USER=${SIP_USER:-ai-assistant}
      - SIP_PASSWORD=${SIP_PASSWORD:-}
      - SIP_DOMAIN=${SIP_DOMAIN:-localhost}
      - SIP_PORT=${SIP_PORT:-5060}
      - SIP_REGISTRAR=${SIP_REGISTRAR:-}
      # General
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
    ports:
      - "5060:5060/udp"
      - "5060:5060/tcp"
      - "10000-10100:10000-10100/udp"  # RTP ports
    volumes:
      - ./data:/app/data
    restart: unless-stopped


# ============================================================================
# Networks
# ============================================================================
networks:
  default:
    driver: bridge